# Kasparro Backend & ETL System

A production-ready ETL pipeline and REST API built with FastAPI, async SQLAlchemy, and PostgreSQL. The system ingests cryptocurrency data from the CoinPaprika API, performs intelligent data normalization with ticker unification, and exposes queryable endpoints with comprehensive metadata.

**Version:** 1.1.2  
**Status:** âœ… Live on Railway  
**Live Demo:** https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/docs  
**Repository:** https://github.com/LikithsaiKovi/kasparro-backend-VenkataLikithSai-Kovi

---

## ğŸš€ Quick Start for Recruiters

**Choose your testing method:**

### Option 1: Test Live Production (No Setup Required) âš¡

**Test in your browser (fastest):**
- ğŸ“Š [Interactive API Docs](https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/docs)
- âœ… [Health Check](https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/health)
- ğŸ’° [View Data](https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/data?limit=5)
- ğŸ“ˆ [Statistics](https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/stats)

**Or test via command line (Windows/Mac/Linux):**
```bash
# Health check
curl https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/health

# View cryptocurrency data
curl https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/data?limit=5

# View ETL statistics
curl https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/stats
```

### Option 2: Run Locally with Docker ğŸ³

**Prerequisites:** Docker Desktop installed and running

```bash
# 1. Clone repository
git clone https://github.com/LikithsaiKovi/kasparro-backend-VenkataLikithSai-Kovi.git
cd kasparro-backend-VenkataLikithSai-Kovi

# 2. Start services (automatically sets up database)
docker-compose up --build

# 3. Test in a new terminal
curl http://localhost:8000/health
curl http://localhost:8000/data?limit=5

# 4. Open browser to http://localhost:8000/docs
```

**That's it!** No manual database setup required.

---

## ğŸ—ï¸ System Architecture & Data Flow

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           EXTERNAL DATA SOURCES                                  â”‚
â”‚                                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ CoinPaprika  â”‚      â”‚  CoinGecko   â”‚      â”‚   CSV Files  â”‚                 â”‚
â”‚  â”‚     API      â”‚      â”‚     API      â”‚      â”‚              â”‚                 â”‚
â”‚  â”‚              â”‚      â”‚              â”‚      â”‚  (Local/     â”‚                 â”‚
â”‚  â”‚ REST API     â”‚      â”‚ REST API     â”‚      â”‚  Mounted)    â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                      â”‚                      â”‚                          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ETL PIPELINE (Background Process)                         â”‚
â”‚                                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  EXTRACT LAYER                                                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚  â”‚  API Source     â”‚  â”‚  API Source     â”‚  â”‚  CSV Source     â”‚        â”‚   â”‚
â”‚  â”‚  â”‚  Fetcher        â”‚  â”‚  Fetcher        â”‚  â”‚  Reader         â”‚        â”‚   â”‚
â”‚  â”‚  â”‚  (CoinPaprika)  â”‚  â”‚  (CoinGecko)    â”‚  â”‚  (CSV Parser)   â”‚        â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚                     â”‚                     â”‚                       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                    â”‚                                             â”‚
â”‚                                    â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TRANSFORM LAYER                                                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  1. Schema Validation (Pydantic Models)                         â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  2. Ticker Normalization (Uppercase, Strip)                     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  3. Price Precision (8 decimal places)                          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  4. Data Type Conversion                                        â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  NORMALIZATION ENGINE (Best-Practice Merging)                   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Merge multi-source records by ticker                         â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Volatile fields: Use most recent (by created_at)             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Static fields: Canonical source priority                     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Preserve one record per ticker                               â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                             â”‚
â”‚                                    â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LOAD LAYER                                                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   â”‚
â”‚  â”‚  â”‚  Raw Data       â”‚              â”‚  Normalized     â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚  Storage        â”‚              â”‚  Data Storage   â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚  (Audit Trail)  â”‚              â”‚  (Queryable)    â”‚                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   â”‚
â”‚  â”‚                                                                         â”‚   â”‚
â”‚  â”‚  â€¢ Idempotent Upserts (ON CONFLICT)                                    â”‚   â”‚
â”‚  â”‚  â€¢ Checkpoint Tracking (Incremental Processing)                        â”‚   â”‚
â”‚  â”‚  â€¢ Run History Logging (Observability)                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      POSTGRESQL DATABASE (Data Warehouse)                        â”‚
â”‚                                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RAW DATA TABLES (Audit & Reprocessing)                                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚  â”‚  â”‚ raw_api_records    â”‚      â”‚ raw_csv_records    â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ external_id (PK) â”‚      â”‚ â€¢ external_id (PK) â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ payload (JSONB)  â”‚      â”‚ â€¢ payload (JSONB)  â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ ingested_at      â”‚      â”‚ â€¢ ingested_at      â”‚                    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  NORMALIZED DATA (Unified Schema)                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ normalized_records                                              â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ id (PK): merged_{ticker}                                      â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ ticker (indexed, uppercase)                                   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ name, price_usd, market_cap_usd, volume_24h_usd              â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ percent_change_24h, source, created_at, ingested_at           â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ ONE RECORD PER TICKER (merged from all sources)               â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  METADATA TABLES (ETL Management)                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚  â”‚  â”‚ etl_checkpoints    â”‚      â”‚ etl_runs           â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ source (unique)  â”‚      â”‚ â€¢ source, status   â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ last_id          â”‚      â”‚ â€¢ processed/failed â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ last_timestamp   â”‚      â”‚ â€¢ duration_ms      â”‚                    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â€¢ started_at       â”‚                    â”‚   â”‚
â”‚  â”‚                               â”‚ â€¢ finished_at      â”‚                    â”‚   â”‚
â”‚  â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FASTAPI REST API LAYER                                   â”‚
â”‚                                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  API ROUTES                                                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   â”‚
â”‚  â”‚  â”‚  GET /health â”‚  â”‚  GET /data   â”‚  â”‚  GET /stats  â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ DB status â”‚  â”‚  â€¢ Query     â”‚  â”‚  â€¢ ETL       â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Last ETL  â”‚  â”‚  â€¢ Filter    â”‚  â”‚    metrics   â”‚                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â€¢ Paginate  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   â”‚
â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚   â”‚
â”‚  â”‚  â”‚ POST /trigger-etlâ”‚      â”‚  GET /docs       â”‚                       â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Manual trigger â”‚      â”‚  â€¢ Swagger UI    â”‚                       â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Background job â”‚      â”‚  â€¢ Interactive   â”‚                       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                   â”‚
â”‚  â€¢ Async SQLAlchemy queries                                                      â”‚
â”‚  â€¢ Request/Response validation (Pydantic)                                        â”‚
â”‚  â€¢ Error handling & logging                                                      â”‚
â”‚  â€¢ CORS enabled for cross-origin requests                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              API CLIENTS                                         â”‚
â”‚                                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚   Web        â”‚  â”‚   Mobile     â”‚  â”‚   Analytics  â”‚                         â”‚
â”‚  â”‚  Dashboard   â”‚  â”‚     App      â”‚  â”‚   Platform   â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                                   â”‚
â”‚  â€¢ REST API Consumers                                                            â”‚
â”‚  â€¢ Real-time cryptocurrency data                                                 â”‚
â”‚  â€¢ Filtered & aggregated queries                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Sequence

```
1. ETL Trigger (Scheduled/Manual)
   â†“
2. EXTRACT: Fetch from CoinPaprika API
   â†“
3. TRANSFORM: 
   - Validate with Pydantic schemas
   - Normalize tickers (uppercase)
   - Normalize prices (8 decimals)
   - Map to unified schema
   â†“
4. MERGE (Best-Practice):
   - Check for existing record by ticker
   - Merge volatile fields (use most recent)
   - Merge static fields (use canonical source)
   - Create/Update unified record
   â†“
5. LOAD:
   - Store raw payloads (audit trail)
   - Upsert normalized records (one per ticker)
   - Update checkpoints (incremental processing)
   - Log ETL run metadata
   â†“
6. API Query:
   - Client requests /data?ticker=BTC
   - FastAPI queries normalized_records
   - Returns merged, enriched record
   â†“
7. Response: Single unified record with best data from all sources
```

### Key Design Decisions

1. **Raw + Normalized Pattern**
   - Raw tables preserve original payloads (audit, reprocessing)
   - Normalized table provides unified, queryable schema

2. **Best-Practice Merging**
   - Volatile fields: Most recent wins (by timestamp)
   - Static fields: Canonical source priority
   - One record per ticker (no duplicates)

3. **Incremental Processing**
   - Checkpoints track last processed ID per source
   - Resumes from failure points
   - Reduces redundant processing

4. **Idempotent Operations**
   - All inserts use `ON CONFLICT` clauses
   - Safe to retry ETL runs
   - Exactly-once semantics

5. **Async Architecture**
   - Async SQLAlchemy + asyncpg driver
   - Async HTTP clients (httpx)
   - Maximizes I/O concurrency

---

---

## ğŸ“Š API Endpoints

All endpoints are documented interactively at `/docs` (Swagger UI)

| Endpoint | Method | Description | Example |
|----------|--------|-------------|---------|
| `/` | GET | API information | `curl http://localhost:8000/` |
| `/health` | GET | System health & DB status | `curl http://localhost:8000/health` |
| `/data` | GET | Query cryptocurrency data | `curl http://localhost:8000/data?limit=5` |
| `/stats` | GET | ETL statistics | `curl http://localhost:8000/stats` |
| `/trigger-etl` | POST | Manually trigger ETL | `curl -X POST http://localhost:8000/trigger-etl` |
| `/docs` | GET | Interactive API docs | Open in browser |

**Query Parameters for `/data`:**
- `limit` (int): Number of records (default: 100, max: 1000)
- `offset` (int): Pagination offset (default: 0)
- `ticker` (string): Filter by ticker (e.g., "BTC")
- `source` (string): Filter by source (e.g., "coinpaprika")
- `created_after` (datetime): Filter by creation date
- `created_before` (datetime): Filter by creation date

**Example Queries:**
```bash
# Get Bitcoin data only
curl http://localhost:8000/data?ticker=BTC

# Get data from specific source
curl http://localhost:8000/data?source=coinpaprika

# Pagination (page 2, 10 per page)
curl http://localhost:8000/data?limit=10&offset=10

# Date range
curl "http://localhost:8000/data?created_after=2025-12-01&created_before=2025-12-31"
```

---

## ğŸ—ï¸ Architecture Overview

```
Data Sources (APIs + CSV) â†’ ETL Pipeline â†’ PostgreSQL â†’ REST API â†’ Clients
                              â†“
                    [Extract] â†’ [Transform] â†’ [Normalize] â†’ [Load]
                              â†“
                    Raw Storage + Normalized Storage (one record per ticker)
```

**Key Features:**
- **API ingestion**: CoinPaprika API
- **Intelligent merging**: Combines data from all sources into unified records
- **Data quality**: Ticker normalization (uppercase), price precision (8 decimals)
- **Automated scheduling**: ETL runs every hour via APScheduler
- **Audit trail**: Raw data preserved for reprocessing
- **Async architecture**: High-performance async SQLAlchemy + asyncpg

---

## ğŸ³ Local Development Setup

### Prerequisites
- Docker Desktop installed and running
- Git (optional, for cloning)

### Quick Start

```bash
# Clone repository
git clone https://github.com/LikithsaiKovi/kasparro-backend-VenkataLikithSai-Kovi.git
cd kasparro-backend-VenkataLikithSai-Kovi

# Start all services (database + API)
docker-compose up --build
```

**Wait for these messages:**
```
âœ” Container kasparro-backend-db-1   Created
âœ” Container kasparro-backend-api-1  Created
api-1  | Database initialized successfully
api-1  | INFO:     Uvicorn running on http://0.0.0.0:8000
```

### Test the API

**Open a new terminal and run:**

```bash
# Windows Command Prompt / PowerShell / Mac / Linux - all use same command
curl http://localhost:8000/health
```

**Expected response:**
```json
{
  "status": "ok",
  "database": "connected",
  "timestamp": "2025-12-16T..."
}
```

### View Interactive Docs

Open in browser: http://localhost:8000/docs

### Stop Services

```bash
# Press Ctrl+C in the Docker terminal, then:
docker-compose down
```

---

## ğŸ§ª Testing the System

### Test Checklist

**Production (Railway) - No Setup Required:**
- [ ] Test health: https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/health
- [ ] View data: https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/data?limit=5
- [ ] Check stats: https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/stats
- [ ] Browse docs: https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/docs

**Local (Docker):**
- [ ] Start: `docker-compose up --build`
- [ ] Test health: `curl http://localhost:8000/health`
- [ ] View data: `curl http://localhost:8000/data?limit=5`
- [ ] Check stats: `curl http://localhost:8000/stats`
- [ ] Browse docs: http://localhost:8000/docs

### Sample Test Queries

```bash
# Localhost
curl http://localhost:8000/data?ticker=BTC
curl http://localhost:8000/data?source=coinpaprika
curl http://localhost:8000/stats

# Production
curl https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/data?ticker=BTC
curl https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/data?source=coinpaprika
curl https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/stats
```

**For detailed testing instructions:** [API_TESTING.md](API_TESTING.md)

---

## ğŸ—„ï¸ Database Schema

**Tables:**
- `normalized_records` - Unified cryptocurrency data (one record per ticker)
- `raw_api_records` - Original API payloads (audit trail)
- `etl_checkpoints` - Incremental processing state
- `etl_runs` - ETL execution history

**Normalized Record Fields:**
- `id` - Unique identifier
- `ticker` - Symbol (uppercase, e.g., "BTC")
- `name` - Full name (e.g., "Bitcoin")
- `price_usd` - Current price (8 decimal precision)
- `market_cap_usd` - Market capitalization
- `volume_24h_usd` - 24-hour trading volume
- `percent_change_24h` - 24-hour price change
- `source` - Data source identifier
- `created_at` - Record creation timestamp
- `ingested_at` - ETL ingestion timestamp
  "name": "Kasparro Backend & ETL",
  "version": "1.1.2",
  "status": "running",
  "endpoints": {
    "health": "/health",
    "data": "/data",
    "stats": "/stats",
    "trigger_etl": "/trigger-etl",
    "docs": "/docs"
  }
}
```

**âœ… Verification:** Should return API information with all available endpoints

---

---

## ğŸ”§ Configuration

### Environment Variables

The system uses sensible defaults. Only `DATABASE_URL` is required:

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `DATABASE_URL` | Yes* | - | PostgreSQL connection string |
| `API_SOURCE_KEY` | No | `REPLACE_ME` | API key (optional) |

| `LOG_LEVEL` | No | `INFO` | Logging level |

*Automatically configured in Docker Compose and Railway

**For Docker Compose:** Environment is configured automatically, no `.env` file needed.

---

## ğŸ“ Project Structure

```
kasparro-backend/
â”œâ”€â”€ api/                    # FastAPI application
â”‚   â”œâ”€â”€ main.py            # App initialization + ETL scheduler
â”‚   â”œâ”€â”€ deps.py            # Dependency injection
â”‚   â””â”€â”€ routes/            # API endpoints
â”œâ”€â”€ core/                  # Core configuration
â”‚   â”œâ”€â”€ config.py          # Settings & validation
â”‚   â””â”€â”€ logger.py          # Logging setup
â”œâ”€â”€ ingestion/             # ETL pipeline
â”‚   â”œâ”€â”€ runner.py          # ETL orchestration
â”‚   â”œâ”€â”€ transform.py       # Data transformation
â”‚   â”œâ”€â”€ normalize.py       # Multi-source merging
â”‚   â””â”€â”€ sources/           # Data adapters
â”œâ”€â”€ schemas/               # Pydantic models
â”œâ”€â”€ services/              # Database layer
â”‚   â”œâ”€â”€ db.py              # Connection management
â”‚   â””â”€â”€ models.py          # ORM models
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ docker-compose.yml     # Local development
â”œâ”€â”€ Dockerfile             # Multi-stage build
â”œâ”€â”€ railway.json           # Railway IaC config
â””â”€â”€ requirements.txt       # Dependencies
```

---

## ğŸš¢ Deployment

### Railway (Production)

The app is deployed on Railway with automated CI/CD:

1. **Push to GitHub** â†’ Railway auto-deploys
2. **Infrastructure as Code**: Configuration in `railway.json`
3. **Automatic DATABASE_URL**: Set by Railway PostgreSQL service
4. **ETL Automation**: Runs every hour automatically

**Deployment guide:** [RAILWAY_DEPLOYMENT.md](RAILWAY_DEPLOYMENT.md)

### Local Development

```bash
docker-compose up --build
```

That's it! Everything is configured automatically.

---

## ğŸ“š Documentation

- **[TESTING_INSTRUCTIONS.md](TESTING_INSTRUCTIONS.md)** - Comprehensive testing guide
- **[TESTING_NORMALIZATION.md](TESTING_NORMALIZATION.md)** - Normalization testing
- **[NORMALIZATION.md](NORMALIZATION.md)** - Data normalization details

---

## ğŸ› Troubleshooting

### Local Development

**Issue: "Connection refused"**
```bash
# Check if Docker is running
docker ps

# Check if services started
docker-compose ps
```

**Issue: "curl: command not found"**
```powershell
# Use PowerShell instead (Windows)
Invoke-WebRequest -Uri http://localhost:8000/health | Select-Object -ExpandProperty Content
```

### Production

**Issue: "database disconnected"**
- Ensure Railway PostgreSQL service is running and linked to your app
- Verify DATABASE_URL environment variable is set in Railway
- Check Railway logs: `railway logs`

---

## âœ… System Verification

**Verified Features:**
- âœ… Multi-source ETL (API + CSV ingestion)
- âœ… Automated scheduling (hourly runs)
- âœ… Data normalization (ticker unification)
- âœ… REST API with filtering & pagination
- âœ… Interactive documentation (Swagger UI)
- âœ… Docker containerization
- âœ… Production deployment on Railway
- âœ… Multi-stage Docker build (optimized)
- âœ… Infrastructure as Code

**Production Status:** âœ… Live on Railway  
**Last Updated:** December 2025  
**Version:** 1.1.2

---

## ğŸ‘¤ Author

Venkata Likith Sai Kovi

**Links:**
- ğŸŒ Live Demo: https://kasparro-backend-venkatalikithsai-kovi-production.up.railway.app/docs
- ğŸ’» Repository: https://github.com/LikithsaiKovi/kasparro-backend-VenkataLikithSai-Kovi

