# Kasparro Backend & ETL System

A production-ready ETL pipeline and REST API built with FastAPI, async SQLAlchemy, and PostgreSQL. The system ingests cryptocurrency data from multiple sources (CoinPaprika API, CoinGecko API, and CSV), stores raw data, performs schema normalization with ticker unification and price precision handling, and exposes queryable endpoints with comprehensive metadata and observability.

**Version:** 1.1.2

**Status:** âœ… Fully Functional & Tested

---

## ğŸš€ Quick Start & Testing Guide

This guide provides step-by-step commands to verify the system works correctly for recruiters/reviewers.

---

## Prerequisites

- **Docker Desktop** installed and running
- **Command Prompt** (Windows) or Terminal (Mac/Linux)
- **Git** (optional, for cloning)

---

## Step 1: Clone & Setup

```bash
# Clone repository
git clone https://github.com/LikithsaiKovi/kasparro-backend-VenkataLikithSai-Kovi.git
cd kasparro-backend-VenkataLikithSai-Kovi

# Create environment file
copy .env.example .env
```

---

## Step 2: Configure Environment Variables

Edit `.env` file with the following (minimum required):

```bash
DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/postgres
API_SOURCE_KEY=REPLACE_ME
CSV_PATH=/data/sample.csv
LOG_LEVEL=INFO
SCHEDULER_TOKEN=your-secure-token-here
```

**Important Notes:**
- `DATABASE_URL` should point to `db:5432` (Docker service name)
- `SCHEDULER_TOKEN` is required for `/trigger-etl` endpoint
- Do NOT add `POSTGRES_PASSWORD`, `POSTGRES_USER`, or `POSTGRES_DB` to `.env` (these are only for docker-compose.yml)

---

## Step 3: Start Services

```bash
# Start PostgreSQL + API services
docker-compose up --build
```

**Expected Output:**
```
âœ” Container kasparro-backend-db-1   Created
âœ” Container kasparro-backend-api-1  Created
db-1  | LOG:  database system is ready to accept connections
api-1  | Database initialized successfully
api-1  | INFO:     Uvicorn running on http://0.0.0.0:8000
```

**âœ… Success Indicators:**
- Database container shows "ready to accept connections"
- API shows "Database initialized successfully"
- API shows "Uvicorn running on http://0.0.0.0:8000"

**Keep this terminal window running!**

---

## Step 4: Verify System Status

Open a **NEW** terminal/command prompt window and run:

### 4.1 Health Check

```bash
curl http://localhost:8000/health
```

**Expected Response:**
```json
{
  "status": "ok",
  "database": "connected",
  "last_etl": "2025-12-14T06:02:32",
  "last_etl_status": "success",
  "timestamp": "2025-12-14T06:05:00Z"
}
```

**âœ… Verification:** Status should be "ok" and database should be "connected"

---

### 4.2 Root Endpoint (API Information)

```bash
curl http://localhost:8000/
```

**Expected Response:**
```json
{
  "name": "Kasparro Backend & ETL",
  "version": "1.1.2",
  "status": "running",
  "endpoints": {
    "health": "/health",
    "data": "/data",
    "stats": "/stats",
    "trigger_etl": "/trigger-etl",
    "docs": "/docs"
  }
}
```

**âœ… Verification:** Should return API information with all available endpoints

---

## Step 5: Test Data Endpoints

### 5.1 View Data (Initial - May be Empty)

```bash
curl http://localhost:8000/data?limit=5
```

**Expected Response:**
```json
{
  "data": [],
  "pagination": {
    "limit": 5,
    "offset": 0,
    "returned": 0,
    "total": 0
  },
  "meta": {
    "request_id": "req-...",
    "api_latency_ms": 12.45
  }
}
```

**âœ… Verification:** Should return valid JSON structure (data may be empty initially)

---

### 5.2 View Statistics (Before ETL Run)

```bash
curl http://localhost:8000/stats
```

**Expected Response:**
```json
{
  "total_normalized": 0,
  "last_success": {
    "source": null,
    "finished_at": null,
    "duration_ms": null,
    "processed": null
  },
  "last_failure": {
    "source": null,
    "finished_at": null,
    "message": null
  }
}
```

**âœ… Verification:** Should return statistics structure (may show zeros initially)

---

## Step 6: Trigger ETL Manually

### 6.1 Trigger ETL (Requires Authentication Token)

```bash
curl -X POST http://localhost:8000/trigger-etl -H "X-Scheduler-Token: your-secure-token-here"
```

**Replace `your-secure-token-here` with your actual `SCHEDULER_TOKEN` from `.env` file**

**Expected Response:**
```json
{
  "status": "triggered",
  "timestamp": "2025-12-14T06:10:00.123456Z"
}
```

**âœ… Verification:** Should return "triggered" status with timestamp

**Wait 15-30 seconds for ETL to process data**

---

### 6.2 Verify ETL Completed

Check the Docker Compose terminal for:
```
api-1  | ETL run completed successfully
api-1  | Processed: 50 records, Failed: 0
```

**âœ… Verification:** Should show successful ETL run in logs

---

## Step 7: Verify Data Ingestion

### 7.1 View Data After ETL

```bash
curl http://localhost:8000/data?limit=10
```

**Expected Response:**
```json
{
  "data": [
    {
      "id": "coinpaprika_btc-bitcoin",
      "ticker": "BTC",
      "name": "Bitcoin",
      "price_usd": 45000.50000000,
      "market_cap_usd": 880000000000.0,
      "volume_24h_usd": 25000000000.0,
      "percent_change_24h": 2.5,
      "source": "coinpaprika",
      "created_at": "2025-12-14T06:00:00Z",
      "ingested_at": "2025-12-14T06:10:00Z"
    },
    ...
  ],
  "pagination": {
    "limit": 10,
    "offset": 0,
    "returned": 10,
    "total": 50
  },
  "meta": {
    "request_id": "req-...",
    "api_latency_ms": 15.23
  }
}
```

**âœ… Verification:** Should return cryptocurrency records with:
- Ticker symbols (normalized to uppercase: BTC, ETH, etc.)
- Price data (normalized to 8 decimal places)
- Source information (coinpaprika, csv)
- Valid timestamps

---

### 7.2 View Updated Statistics

```bash
curl http://localhost:8000/stats
```

**Expected Response:**
```json
{
  "total_normalized": 50,
  "last_success": {
    "source": "api",
    "finished_at": "2025-12-14T06:10:15Z",
    "duration_ms": 12450,
    "processed": 45
  },
  "last_failure": {
    "source": null,
    "finished_at": null,
    "message": null
  }
}
```

**âœ… Verification:** Should show:
- `total_normalized` > 0
- `last_success` with recent timestamp
- `processed` count > 0

---

## Step 8: Test Filtering & Query Features

### 8.1 Filter by Ticker

```bash
curl "http://localhost:8000/data?ticker=BTC"
```

**Expected Response:** Only BTC records

**âœ… Verification:** Should return only records with ticker "BTC"

---

### 8.2 Filter by Source

```bash
curl "http://localhost:8000/data?source=coinpaprika&limit=5"
```

**Expected Response:** Only records from CoinPaprika API

**âœ… Verification:** All returned records should have `"source": "coinpaprika"`

---

### 8.3 Test Pagination

```bash
# First page
curl "http://localhost:8000/data?limit=2&offset=0"

# Second page
curl "http://localhost:8000/data?limit=2&offset=2"

# Third page
curl "http://localhost:8000/data?limit=2&offset=4"
```

**âœ… Verification:** 
- Each page should return 2 records
- Records should be different (no duplicates across pages)
- `offset` should increase by `limit` value

---

### 8.4 Test Date Filtering (Optional)

```bash
curl "http://localhost:8000/data?start_date=2025-12-14T00:00:00Z&end_date=2025-12-14T23:59:59Z"
```

**âœ… Verification:** Should return records within the date range

---

## Step 9: Interactive API Documentation

Open in web browser:
```
http://localhost:8000/docs
```

**âœ… Verification:** Should display Swagger UI with:
- All endpoints listed
- Try it out functionality
- Request/response schemas
- Authentication options

**Test endpoints directly from the browser interface**

---

## Step 10: Verify Database Tables

### 10.1 Connect to PostgreSQL Container

```bash
docker exec -it kasparro-backend-db-1 psql -U postgres -d postgres
```

### 10.2 Check Tables Exist

```sql
\dt
```

**Expected Output:**
```
                    List of relations
 Schema |         Name          | Type  |  Owner
--------+-----------------------+-------+----------
 public | raw_api_records       | table | postgres
 public | raw_csv_records       | table | postgres
 public | normalized_records    | table | postgres
 public | etl_checkpoints       | table | postgres
 public | etl_runs              | table | postgres
```

**âœ… Verification:** All 5 tables should exist

---

### 10.3 Check Data in Tables

```sql
-- Check normalized records
SELECT COUNT(*) FROM normalized_records;

-- Check raw API records
SELECT COUNT(*) FROM raw_api_records;

-- Check raw CSV records
SELECT COUNT(*) FROM raw_csv_records;

-- Check ETL runs
SELECT source, status, processed, failed, finished_at FROM etl_runs ORDER BY finished_at DESC LIMIT 5;

-- Check checkpoints
SELECT source, last_id, updated_at FROM etl_checkpoints;
```

**Expected Output:**
- `normalized_records` should have data (> 0 after ETL)
- `raw_api_records` should have data
- `raw_csv_records` should have data
- `etl_runs` should show successful runs
- `etl_checkpoints` should show last processed IDs

**âœ… Verification:** All tables should have data after ETL runs

```sql
-- Exit PostgreSQL
\q
```

---

## Step 11: Complete Testing Checklist

Use this checklist to verify all functionality:

### âœ… Core Functionality
- [ ] Docker Compose starts successfully
- [ ] Database initializes correctly
- [ ] API server starts and responds
- [ ] Health endpoint returns "ok" status
- [ ] Database connection is "connected"

### âœ… Data Operations
- [ ] ETL trigger endpoint works (with authentication)
- [ ] ETL processes data successfully
- [ ] Data appears in `/data` endpoint after ETL
- [ ] Raw data is stored (API and CSV sources)
- [ ] Normalized data is created correctly

### âœ… Query Features
- [ ] Filtering by ticker works
- [ ] Filtering by source works
- [ ] Pagination works (limit/offset)
- [ ] Date filtering works (if tested)
- [ ] Statistics endpoint shows correct counts

### âœ… Data Quality
- [ ] Tickers are normalized to uppercase
- [ ] Prices are displayed with proper precision
- [ ] Source information is correct
- [ ] Timestamps are valid ISO format

### âœ… Documentation
- [ ] Interactive API docs accessible at `/docs`
- [ ] All endpoints are documented
- [ ] Request/response schemas are visible

---

## Step 12: Stop Services

```bash
# In the Docker Compose terminal, press Ctrl+C
# Then run:
docker-compose down
```

**Expected Output:**
```
Gracefully stopping... done
âœ” Container kasparro-backend-api-1  Removed
âœ” Container kasparro-backend-db-1   Removed
âœ” Network kasparro-backend_default  Removed
```

---

## Troubleshooting

### Issue: "Invalid scheduler token"
**Solution:** Include the token header:
```bash
curl -X POST http://localhost:8000/trigger-etl -H "X-Scheduler-Token: YOUR_TOKEN_FROM_ENV"
```

### Issue: "curl is not recognized"
**Solution:** Use PowerShell:
```powershell
Invoke-WebRequest -Uri http://localhost:8000/health | Select-Object -ExpandProperty Content
```

### Issue: "Connection refused"
**Solution:** Ensure Docker Compose is running:
```bash
docker-compose ps
# Should show both api-1 and db-1 as "Up"
```

### Issue: Database connection failed
**Solution:** Check DATABASE_URL in `.env`:
```bash
# Should be:
DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/postgres
# NOT: postgres-9ni8.railway.internal (that's for Railway deployment)
```

### Issue: "Extra inputs are not permitted"
**Solution:** Remove `POSTGRES_PASSWORD`, `POSTGRES_USER`, `POSTGRES_DB` from `.env` file (these belong only in docker-compose.yml)

---

## Alternative: Testing with PowerShell (Windows)

If `curl` doesn't work, use PowerShell commands:

```powershell
# Health check
Invoke-WebRequest -Uri http://localhost:8000/health | Select-Object -ExpandProperty Content

# View data
Invoke-WebRequest -Uri "http://localhost:8000/data?limit=5" | Select-Object -ExpandProperty Content

# View stats
Invoke-WebRequest -Uri http://localhost:8000/stats | Select-Object -ExpandProperty Content

# Trigger ETL (with token)
$headers = @{"X-Scheduler-Token" = "your-secure-token-here"}
Invoke-WebRequest -Uri http://localhost:8000/trigger-etl -Method POST -Headers $headers | Select-Object -ExpandProperty Content
```

---

## Production Deployment (Railway)

The application is also deployed on Railway. To test the production deployment:

1. Get the Railway deployment URL from Railway dashboard
2. Replace `localhost:8000` with your Railway URL in all commands above
3. Example: `curl https://your-app.railway.app/health`

**Note:** Railway deployment uses Railway-managed PostgreSQL database.

---

## ğŸ“‹ Project Structure

```
kasparro-backend/
â”œâ”€â”€ api/                    # FastAPI application
â”‚   â”œâ”€â”€ main.py            # App initialization
â”‚   â”œâ”€â”€ deps.py            # Dependency injection
â”‚   â””â”€â”€ routes/            # API endpoints
â”‚       â”œâ”€â”€ data.py        # GET /data
â”‚       â”œâ”€â”€ health.py      # GET /health
â”‚       â”œâ”€â”€ stats.py       # GET /stats
â”‚       â””â”€â”€ trigger.py     # POST /trigger-etl
â”œâ”€â”€ core/                  # Core configuration
â”‚   â”œâ”€â”€ config.py          # Environment settings
â”‚   â””â”€â”€ logger.py          # Logging configuration
â”œâ”€â”€ ingestion/             # ETL pipeline
â”‚   â”œâ”€â”€ runner.py          # ETL orchestration
â”‚   â”œâ”€â”€ transform.py       # Schema transformation
â”‚   â””â”€â”€ sources/           # Data source adapters
â”‚       â”œâ”€â”€ api_source.py  # CoinPaprika API
â”‚       â””â”€â”€ csv_source.py  # CSV file reader
â”œâ”€â”€ schemas/               # Pydantic models
â”‚   â””â”€â”€ record.py          # Data validation schemas
â”œâ”€â”€ services/              # Database layer
â”‚   â”œâ”€â”€ db.py              # Database connection
â”‚   â””â”€â”€ models.py          # SQLAlchemy ORM models
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ data/                  # Sample CSV files
â”œâ”€â”€ docker-compose.yml     # Docker services
â”œâ”€â”€ Dockerfile             # Container definition
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ start.sh               # Container entrypoint
```

---

## ğŸ—„ï¸ Database Schema

### Tables

- **raw_api_records**: Original API payloads (audit trail)
- **raw_csv_records**: Original CSV records (audit trail)
- **normalized_records**: Unified, queryable schema (one record per ticker)
- **etl_checkpoints**: Incremental processing state
- **etl_runs**: ETL execution audit log

### Normalized Records Schema

- `id`: Primary key (source-specific identifier)
- `ticker`: Cryptocurrency ticker (normalized to uppercase)
- `name`: Full cryptocurrency name
- `price_usd`: Price in USD (normalized to 8 decimal places)
- `market_cap_usd`: Market capitalization
- `volume_24h_usd`: 24-hour trading volume
- `percent_change_24h`: 24-hour price change percentage
- `source`: Data source (coinpaprika, coingecko, csv)
- `created_at`: Source record creation time
- `ingested_at`: When record was saved

**Normalization Logic:** One record per ticker (unification by overwrite - last writer wins)

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `DATABASE_URL` | Yes | - | PostgreSQL async connection string |
| `API_SOURCE_KEY` | No | `REPLACE_ME` | API authentication key (optional) |
| `CSV_PATH` | No | `data/sample.csv` | Path to CSV file |
| `LOG_LEVEL` | No | `INFO` | Logging verbosity |
| `SCHEDULER_TOKEN` | No | - | Token for `/trigger-etl` authentication |

---

## ğŸ§ª Automated Testing

```bash
# Run tests
docker-compose run --rm api pytest -v

# With coverage
docker-compose run --rm api pytest --cov=. --cov-report=html
```

---

## ğŸ“ Notes

- The system uses async SQLAlchemy with `asyncpg` for PostgreSQL connections
- Raw data is preserved for audit trail and reprocessing
- ETL uses checkpointing for incremental processing
- All timestamps are stored in UTC
- Ticker symbols are normalized to uppercase
- Prices are normalized to 8 decimal places
- Normalization ensures one record per cryptocurrency ticker

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¤ Author

Venkata Likith Sai Kovi

---

## ğŸ”— Links

- **Interactive API Documentation:** http://localhost:8000/docs (when running locally)
- **Testing Instructions:** [TESTING_INSTRUCTIONS.md](TESTING_INSTRUCTIONS.md)
- **Normalization Details:** [NORMALIZATION.md](NORMALIZATION.md)
- **GitHub Repository:** https://github.com/LikithsaiKovi/kasparro-backend-VenkataLikithSai-Kovi

---

## âœ… System Verification Summary

This system has been tested and verified to work correctly:

- âœ… All API endpoints functional
- âœ… Database operations working
- âœ… ETL pipeline ingesting data correctly
- âœ… Data normalization working (one record per ticker)
- âœ… Filtering and pagination functional
- âœ… Error handling implemented
- âœ… Docker deployment working
- âœ… Production deployment on Railway functional

**Last Verified:** December 2024  
**Version:** 1.1.2
